{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras import Sequential\nfrom keras.layers import LSTM, Dense\nfrom sklearn.model_selection import KFold, StratifiedKFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_flow = None\nfor i in range(5):\n    data_day = pd.read_csv('/kaggle/input/traffic-prediction-data/d11_text_station_5min_2019_09_0'+str(i+1)+'.csv',header=None)\n    data_day = data_day.iloc[:,[0,1,9]]\n    data_day.columns=['time','station_id','flow']\n    data_flow=pd.concat([data_flow,data_day],ignore_index=True)\ndata_flow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_flow_1100326 = data_flow.loc[data_flow['station_id']==1100326].dropna(subset=['flow'])\nplt.plot(data_flow_1100326['time'],data_flow_1100326['flow'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_flow=data_flow.groupby('station_id').filter(lambda x : x.flow.count()!=0)\ndata_flow.fillna(method='pad',axis=0,inplace=True)\ngrouped=data_flow.groupby('time')\ngrouped.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=[]\nfor name, group in grouped:\n    data.append(group['flow'].tolist())\ndata=np.array(data)\npd.DataFrame(data)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def max_min_normalization(x, _max, _min):\n    x = 1. * (x - _min)/(_max - _min)\n    x = x * 2. - 1.\n    return x\n\n\ndef re_max_min_normalization(x, _max, _min):\n    x = (x + 1.) / 2.\n    x = 1. * x * (_max - _min) + _min\n    return x\ndef min_max(data_seq):\n    all_data=[]\n    for data in data_seq:\n        min=np.min(data)\n        max=np.max(data)\n        for i in range(0,len(data)):\n            data[i]=max_min_normalization(data[i],max,min)\n        all_data.append(data)\n    return all_data\n\ndef re_min_max(data_seq):\n    all_data=[]\n    for data in data_seq:\n        min=np.min(data)\n        max=np.max(data)\n        for i in range(0,len(data)):\n            data[i]=re_max_min_normalization(data[i],max,min)\n        all_data.append(data)\n    return all_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler=MinMaxScaler(feature_range=(0,1))\ndata_input=scaler.fit_transform(data)\npd.DataFrame(data_input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_and_generate_dataset(data_seq, num_of_depend):\n    if len(data_seq) <= num_of_depend:\n        return None\n    X_data = []\n    y_data = []\n    for i in range(num_of_depend, len(data_seq)):\n        X_start_idx = i - num_of_depend\n        X_data.append(data_seq[X_start_idx:i])\n        y_data.append(data_seq[i])\n    val_line = int(len(X_data)*0.6)\n    test_line = int(len(X_data)*0.8)\n    return np.array(X_data[:val_line]),np.array(X_data[val_line:test_line]),np.array(X_data[test_line:]),np.array(y_data[:val_line]),np.array(y_data[val_line:test_line]),np.array(y_data[test_line:])\nX_train, X_val,X_test, y_train,y_val, y_test = read_and_generate_dataset(data_input, 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_shape = X_train.shape\ndata_shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=3, shuffle=True)\nfold_no = 1\ncv_scores = []\nfor train, test in kfold.split(X_train, y_train):\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n    model = Sequential()\n    # model.add(Conv1D(filters=128, kernel_size=10, padding='same', strides=1, activation='relu',input_shape=(data_shape[1],data_shape[2])))\n    model.add(LSTM(128, input_shape=(data_shape[1], data_shape[2]),activation='relu',return_sequences = True))\n    model.add(LSTM(128, input_shape=(data_shape[1], 128),activation='relu'))\n    model.add(Dense(data_shape[2],activation='relu'))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    history = model.fit(X_train, \n                        y_train, \n                        epochs=30, \n                        batch_size=32, \n                        verbose=2, \n                        shuffle=True)\n\n    # Generate generalization metrics\n    score = model.evaluate(X_train[test], y_train[test], verbose=0)\n    cv_scores.append(score)\n    # Increase fold number\n    fold_no = fold_no + 1\n\npd.DataFrame(cv_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_scores_df = pd.DataFrame(cv_scores)\ncv_scores_df.columns=['cv_score']\ncv_scores_df.index=['cv1','cv2','cv3']\ncv_scores_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(128, input_shape=(data_shape[1], data_shape[2]),activation='relu',return_sequences = True))\nmodel.add(LSTM(128, input_shape=(data_shape[1], 128),activation='relu'))\nmodel.add(Dense(data_shape[2],activation='relu'))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n# fit network\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=2, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = model.evaluate(X_train[test], y_train[test], verbose=0)\nscore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='val')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(X_test)\nprediction = scaler.inverse_transform(prediction)\n\ny_test=scaler.inverse_transform(y_test)\n\nprediction_s = prediction[:,0]\ny_test_s = y_test[:,0]\ny_train_s=scaler.inverse_transform(y_train)[:,0]\ny_val_s=scaler.inverse_transform(y_val)[:,0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w=3\nx1=np.linspace(0,y_train_s.shape[0]*w,y_train_s.shape[0])\nx2=np.linspace(y_train_s.shape[0]*w,(y_train_s.shape[0]+y_val_s.shape[0])*w,y_val_s.shape[0])\nx3=np.linspace((y_train_s.shape[0]+y_val_s.shape[0])*w,(y_train_s.shape[0]+y_val_s.shape[0]+y_test_s.shape[0])*w,y_test_s.shape[0])\n# plt.plot(x1,y_train_s)\n# plt.plot(x2,y_val_s)\nplt.plot(x3,prediction_s,'r')\nplt.plot(x3,y_test_s,'b')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}